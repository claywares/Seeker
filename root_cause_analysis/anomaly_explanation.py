"""
å¼‚å¸¸æ ¹å› åˆ†æä¸è§£é‡Šæ€§
ä¸ºæ£€æµ‹åˆ°çš„å¼‚å¸¸æä¾›å¯è§£é‡Šçš„åŸå› åˆ†æ
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestRegressor
from sklearn.inspection import permutation_importance
import shap
from typing import Dict, List, Tuple, Any
from dataclasses import dataclass
import warnings
warnings.filterwarnings('ignore')

@dataclass
class AnomalyExplanation:
    """å¼‚å¸¸è§£é‡Šç»“æœ"""
    timestamp: pd.Timestamp
    value: float
    anomaly_score: float
    primary_cause: str
    contributing_factors: List[Tuple[str, float]]  # (å› å­åç§°, è´¡çŒ®åº¦)
    pattern_type: str  # 'spike', 'drop', 'drift', 'oscillation'
    context: Dict[str, Any]

class RootCauseAnalyzer:
    """å¼‚å¸¸æ ¹å› åˆ†æå™¨"""
    
    def __init__(self):
        self.feature_importance = {}
        self.historical_patterns = {}
        self.context_analyzers = []
        
    def extract_features(self, data: pd.Series, index: int, window_size: int = 10) -> Dict[str, float]:
        """
        æå–å¼‚å¸¸ç‚¹å‘¨å›´çš„ç‰¹å¾
        
        Args:
            data: æ—¶é—´åºåˆ—æ•°æ®
            index: å¼‚å¸¸ç‚¹ç´¢å¼•
            window_size: åˆ†æçª—å£å¤§å°
        """
        start_idx = max(0, index - window_size)
        end_idx = min(len(data), index + window_size + 1)
        
        # åŸºç¡€çª—å£æ•°æ®
        window_data = data.iloc[start_idx:end_idx]
        before_window = data.iloc[max(0, start_idx-window_size):start_idx] if start_idx > 0 else pd.Series([])
        
        features = {}
        
        # 1. ç»Ÿè®¡ç‰¹å¾
        features['current_value'] = data.iloc[index]
        features['window_mean'] = window_data.mean()
        features['window_std'] = window_data.std()
        features['window_median'] = window_data.median()
        features['window_range'] = window_data.max() - window_data.min()
        
        # 2. ç›¸å¯¹ä½ç½®ç‰¹å¾
        if len(before_window) > 0:
            features['relative_to_before'] = data.iloc[index] / (before_window.mean() + 1e-6)
            features['change_from_before'] = data.iloc[index] - before_window.mean()
        else:
            features['relative_to_before'] = 1.0
            features['change_from_before'] = 0.0
        
        # 3. å±€éƒ¨è¶‹åŠ¿ç‰¹å¾
        if index >= 3:
            recent_trend = np.polyfit(range(3), data.iloc[index-2:index+1], 1)[0]
            features['recent_trend'] = recent_trend
        else:
            features['recent_trend'] = 0.0
        
        # 4. å˜åŒ–ç‡ç‰¹å¾
        if index > 0:
            features['rate_of_change'] = data.iloc[index] - data.iloc[index-1]
            features['acceleration'] = features['rate_of_change'] - (
                data.iloc[index-1] - data.iloc[index-2] if index > 1 else 0
            )
        else:
            features['rate_of_change'] = 0.0
            features['acceleration'] = 0.0
        
        # 5. å‘¨æœŸæ€§ç‰¹å¾
        for period in [7, 24, 144]:  # å‘¨ã€æ—¥ã€å°æ—¶å‘¨æœŸ
            if index >= period:
                seasonal_value = data.iloc[index - period]
                features[f'seasonal_diff_{period}'] = data.iloc[index] - seasonal_value
                features[f'seasonal_ratio_{period}'] = data.iloc[index] / (seasonal_value + 1e-6)
            else:
                features[f'seasonal_diff_{period}'] = 0.0
                features[f'seasonal_ratio_{period}'] = 1.0
        
        # 6. å±€éƒ¨æå€¼ç‰¹å¾
        local_data = window_data
        features['is_local_max'] = float(data.iloc[index] == local_data.max())
        features['is_local_min'] = float(data.iloc[index] == local_data.min())
        features['local_rank'] = (local_data < data.iloc[index]).sum() / len(local_data)
        
        # 7. æ³¢åŠ¨æ€§ç‰¹å¾
        if len(window_data) > 2:
            features['local_volatility'] = window_data.rolling(3).std().mean()
        else:
            features['local_volatility'] = 0.0
        
        return features
    
    def identify_pattern_type(self, data: pd.Series, index: int, window_size: int = 5) -> str:
        """è¯†åˆ«å¼‚å¸¸æ¨¡å¼ç±»å‹"""
        value = data.iloc[index]
        
        # è·å–å‘¨å›´æ•°æ®
        start_idx = max(0, index - window_size)
        end_idx = min(len(data), index + window_size + 1)
        surrounding = data.iloc[start_idx:end_idx]
        
        # è®¡ç®—ç»Ÿè®¡é‡
        mean_surrounding = surrounding.mean()
        std_surrounding = surrounding.std()
        
        # æ¨¡å¼è¯†åˆ«
        if value > mean_surrounding + 2 * std_surrounding:
            if index > 0 and index < len(data) - 1:
                # æ£€æŸ¥æ˜¯å¦ä¸ºå°–å³°
                before = data.iloc[index - 1] if index > 0 else mean_surrounding
                after = data.iloc[index + 1] if index < len(data) - 1 else mean_surrounding
                if abs(value - before) > std_surrounding and abs(value - after) > std_surrounding:
                    return 'spike'
            return 'high_value'
        
        elif value < mean_surrounding - 2 * std_surrounding:
            if index > 0 and index < len(data) - 1:
                # æ£€æŸ¥æ˜¯å¦ä¸ºå°–è°·
                before = data.iloc[index - 1] if index > 0 else mean_surrounding
                after = data.iloc[index + 1] if index < len(data) - 1 else mean_surrounding
                if abs(value - before) > std_surrounding and abs(value - after) > std_surrounding:
                    return 'drop'
            return 'low_value'
        
        # æ£€æŸ¥è¶‹åŠ¿å˜åŒ–
        if index >= 3:
            recent_values = data.iloc[index-2:index+1]
            if len(recent_values) == 3:
                trend = np.polyfit(range(3), recent_values, 1)[0]
                if abs(trend) > std_surrounding / 2:
                    return 'drift'
        
        # æ£€æŸ¥æŒ¯è¡
        if index >= 2 and index < len(data) - 2:
            local_window = data.iloc[index-2:index+3]
            if len(local_window) == 5:
                changes = local_window.diff().dropna()
                if (changes > 0).sum() >= 2 and (changes < 0).sum() >= 2:
                    return 'oscillation'
        
        return 'unknown'
    
    def analyze_anomaly(self, data: pd.Series, anomaly_index: int, 
                       context_data: Dict[str, pd.Series] = None) -> AnomalyExplanation:
        """
        åˆ†æå•ä¸ªå¼‚å¸¸ç‚¹çš„æ ¹å› 
        
        Args:
            data: ä¸»è¦æ—¶é—´åºåˆ—æ•°æ®
            anomaly_index: å¼‚å¸¸ç‚¹ç´¢å¼•
            context_data: ä¸Šä¸‹æ–‡æ•°æ®ï¼ˆå¦‚å…¶ä»–æŒ‡æ ‡ï¼‰
        """
        # æå–ç‰¹å¾
        features = self.extract_features(data, anomaly_index)
        
        # è¯†åˆ«æ¨¡å¼ç±»å‹
        pattern_type = self.identify_pattern_type(data, anomaly_index)
        
        # è®¡ç®—å¼‚å¸¸åˆ†æ•°
        value = data.iloc[anomaly_index]
        window_data = data.iloc[max(0, anomaly_index-10):anomaly_index+11]
        z_score = abs(value - window_data.mean()) / (window_data.std() + 1e-6)
        
        # åˆ†æä¸»è¦åŸå› 
        primary_cause = self._identify_primary_cause(features, pattern_type)
        
        # è®¡ç®—è´¡çŒ®å› å­
        contributing_factors = self._calculate_contributing_factors(features)
        
        # ä¸Šä¸‹æ–‡åˆ†æ
        context = self._analyze_context(data, anomaly_index, context_data)
        
        return AnomalyExplanation(
            timestamp=data.index[anomaly_index] if hasattr(data.index, '__getitem__') else anomaly_index,
            value=value,
            anomaly_score=min(z_score / 3.0, 1.0),  # æ ‡å‡†åŒ–åˆ°0-1
            primary_cause=primary_cause,
            contributing_factors=contributing_factors,
            pattern_type=pattern_type,
            context=context
        )
    
    def _identify_primary_cause(self, features: Dict[str, float], pattern_type: str) -> str:
        """è¯†åˆ«ä¸»è¦åŸå› """
        # æ ¹æ®ç‰¹å¾å’Œæ¨¡å¼ç±»å‹ç¡®å®šä¸»è¦åŸå› 
        if pattern_type == 'spike':
            if features['rate_of_change'] > features['window_std'] * 2:
                return "çªå‘æ€§å³°å€¼"
            elif features.get('seasonal_diff_24', 0) > features['window_std']:
                return "åç¦»æ—¥å¸¸æ¨¡å¼"
            else:
                return "å±€éƒ¨å¼‚å¸¸å³°å€¼"
        
        elif pattern_type == 'drop':
            if features['rate_of_change'] < -features['window_std'] * 2:
                return "çªå‘æ€§ä¸‹é™"
            elif features.get('seasonal_diff_24', 0) < -features['window_std']:
                return "ä½äºé¢„æœŸæ°´å¹³"
            else:
                return "å±€éƒ¨å¼‚å¸¸ä½å€¼"
        
        elif pattern_type == 'drift':
            if abs(features['recent_trend']) > features['window_std']:
                return "è¶‹åŠ¿å¼‚å¸¸å˜åŒ–"
            else:
                return "æ¸è¿›æ€§åç§»"
        
        elif pattern_type == 'oscillation':
            return "å¼‚å¸¸æŒ¯è¡æ¨¡å¼"
        
        else:
            # åŸºäºç‰¹å¾é‡è¦æ€§åˆ¤æ–­
            if abs(features['relative_to_before'] - 1) > 0.5:
                return "ç›¸å¯¹å†å²æ°´å¹³å¼‚å¸¸"
            elif abs(features['change_from_before']) > features['window_std']:
                return "åç¦»åŸºçº¿æ°´å¹³"
            else:
                return "ç»Ÿè®¡å¼‚å¸¸å€¼"
    
    def _calculate_contributing_factors(self, features: Dict[str, float]) -> List[Tuple[str, float]]:
        """è®¡ç®—è´¡çŒ®å› å­"""
        factor_scores = []
        
        # åŸºäºç‰¹å¾å€¼è®¡ç®—è´¡çŒ®åº¦
        feature_weights = {
            'relative_to_before': 0.25,
            'change_from_before': 0.20,
            'rate_of_change': 0.15,
            'recent_trend': 0.10,
            'local_volatility': 0.10,
            'seasonal_diff_24': 0.20
        }
        
        for feature, weight in feature_weights.items():
            if feature in features:
                # æ ‡å‡†åŒ–ç‰¹å¾å€¼
                normalized_value = min(abs(features[feature]) / (features.get('window_std', 1) + 1e-6), 2.0)
                contribution = weight * normalized_value
                
                # æ˜ å°„åˆ°å¯è¯»åç§°
                readable_name = {
                    'relative_to_before': 'ç›¸å¯¹å†å²æ¯”ä¾‹',
                    'change_from_before': 'åŸºçº¿åç§»',
                    'rate_of_change': 'å˜åŒ–ç‡',
                    'recent_trend': 'çŸ­æœŸè¶‹åŠ¿',
                    'local_volatility': 'å±€éƒ¨æ³¢åŠ¨',
                    'seasonal_diff_24': 'æ—¥å‘¨æœŸåå·®'
                }.get(feature, feature)
                
                factor_scores.append((readable_name, contribution))
        
        # æŒ‰è´¡çŒ®åº¦æ’åº
        factor_scores.sort(key=lambda x: x[1], reverse=True)
        
        return factor_scores[:5]  # è¿”å›å‰5ä¸ªä¸»è¦å› å­
    
    def _analyze_context(self, data: pd.Series, index: int, 
                        context_data: Dict[str, pd.Series] = None) -> Dict[str, Any]:
        """åˆ†æä¸Šä¸‹æ–‡ä¿¡æ¯"""
        context = {}
        
        # æ—¶é—´ä¸Šä¸‹æ–‡
        if hasattr(data.index, '__getitem__') and hasattr(data.index[index], 'hour'):
            timestamp = data.index[index]
            context['hour_of_day'] = timestamp.hour
            context['day_of_week'] = timestamp.weekday()
            context['is_weekend'] = timestamp.weekday() >= 5
            context['is_business_hours'] = 9 <= timestamp.hour <= 17
        
        # å†å²å¯¹æ¯”
        window_data = data.iloc[max(0, index-50):index]
        if len(window_data) > 0:
            context['percentile_in_history'] = (window_data < data.iloc[index]).sum() / len(window_data)
            context['historical_max'] = window_data.max()
            context['historical_min'] = window_data.min()
        
        # å…³è”æŒ‡æ ‡åˆ†æ
        if context_data:
            correlations = {}
            for name, series in context_data.items():
                if index < len(series):
                    # è®¡ç®—çŸ­æœŸç›¸å…³æ€§
                    start_idx = max(0, index - 20)
                    data_window = data.iloc[start_idx:index+1]
                    context_window = series.iloc[start_idx:index+1]
                    
                    if len(data_window) > 3 and len(context_window) > 3:
                        corr = data_window.corr(context_window)
                        if not np.isnan(corr):
                            correlations[name] = corr
            
            context['correlations'] = correlations
        
        return context
    
    def generate_explanation_report(self, explanation: AnomalyExplanation) -> str:
        """ç”Ÿæˆå¯è¯»çš„è§£é‡ŠæŠ¥å‘Š"""
        report = []
        
        report.append(f"ğŸ” å¼‚å¸¸åˆ†ææŠ¥å‘Š")
        report.append(f"=" * 50)
        report.append(f"æ—¶é—´ç‚¹: {explanation.timestamp}")
        report.append(f"å¼‚å¸¸å€¼: {explanation.value:.2f}")
        report.append(f"å¼‚å¸¸åˆ†æ•°: {explanation.anomaly_score:.3f}")
        report.append(f"æ¨¡å¼ç±»å‹: {explanation.pattern_type}")
        report.append("")
        
        report.append(f"ğŸ¯ ä¸»è¦åŸå› : {explanation.primary_cause}")
        report.append("")
        
        report.append(f"ğŸ“Š è´¡çŒ®å› å­:")
        for factor, contribution in explanation.contributing_factors:
            bar_length = int(contribution * 20)
            bar = "â–ˆ" * bar_length + "â–‘" * (20 - bar_length)
            report.append(f"  {factor:.<20} {bar} {contribution:.3f}")
        report.append("")
        
        if explanation.context:
            report.append(f"ğŸ”— ä¸Šä¸‹æ–‡ä¿¡æ¯:")
            for key, value in explanation.context.items():
                if key != 'correlations':
                    report.append(f"  {key}: {value}")
            
            if 'correlations' in explanation.context:
                report.append("  ç›¸å…³æŒ‡æ ‡:")
                for metric, corr in explanation.context['correlations'].items():
                    report.append(f"    {metric}: {corr:.3f}")
        
        return "\n".join(report)
    
    def batch_analyze_anomalies(self, data: pd.Series, anomaly_indices: List[int],
                               context_data: Dict[str, pd.Series] = None) -> List[AnomalyExplanation]:
        """æ‰¹é‡åˆ†æå¤šä¸ªå¼‚å¸¸ç‚¹"""
        explanations = []
        
        for index in anomaly_indices:
            if 0 <= index < len(data):
                explanation = self.analyze_anomaly(data, index, context_data)
                explanations.append(explanation)
        
        return explanations
    
    def plot_anomaly_explanation(self, data: pd.Series, explanation: AnomalyExplanation, 
                                window_size: int = 20):
        """å¯è§†åŒ–å¼‚å¸¸è§£é‡Š"""
        index = explanation.timestamp if isinstance(explanation.timestamp, int) else None
        
        # å¦‚æœtimestampæ˜¯pandasæ—¶é—´æˆ³ï¼Œéœ€è¦æ‰¾åˆ°å¯¹åº”çš„ç´¢å¼•
        if index is None:
            try:
                index = data.index.get_loc(explanation.timestamp)
            except:
                print("æ— æ³•æ‰¾åˆ°å¯¹åº”çš„ç´¢å¼•ä½ç½®")
                return
        
        # ç¡®å®šç»˜å›¾çª—å£
        start_idx = max(0, index - window_size)
        end_idx = min(len(data), index + window_size + 1)
        
        window_data = data.iloc[start_idx:end_idx]
        
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))
        
        # ä¸»å›¾ï¼šæ—¶é—´åºåˆ—å’Œå¼‚å¸¸ç‚¹
        ax1.plot(range(len(window_data)), window_data.values, 
                label='Time Series', alpha=0.7, linewidth=2)
        
        anomaly_pos = index - start_idx
        ax1.scatter([anomaly_pos], [explanation.value], 
                   color='red', s=100, zorder=5, label='Anomaly')
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        mean_val = window_data.mean()
        std_val = window_data.std()
        ax1.axhline(y=mean_val, color='green', linestyle='--', alpha=0.7, label='Mean')
        ax1.axhline(y=mean_val + 2*std_val, color='orange', linestyle=':', alpha=0.7, label='Â±2Ïƒ')
        ax1.axhline(y=mean_val - 2*std_val, color='orange', linestyle=':', alpha=0.7)
        
        ax1.set_title(f'Anomaly Context: {explanation.primary_cause}')
        ax1.set_ylabel('Value')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # ä¸‹å›¾ï¼šè´¡çŒ®å› å­
        factors = [f[0] for f in explanation.contributing_factors]
        contributions = [f[1] for f in explanation.contributing_factors]
        
        bars = ax2.barh(factors, contributions, color='skyblue', alpha=0.7)
        ax2.set_xlabel('Contribution Score')
        ax2.set_title('Contributing Factors')
        ax2.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, contribution in zip(bars, contributions):
            width = bar.get_width()
            ax2.text(width + 0.01, bar.get_y() + bar.get_height()/2, 
                    f'{contribution:.3f}', ha='left', va='center')
        
        plt.tight_layout()
        plt.savefig('root_cause_analysis/anomaly_explanation.png', dpi=300, bbox_inches='tight')
        plt.show()

# ç¤ºä¾‹ç”¨æ³•
if __name__ == "__main__":
    # ç”Ÿæˆç¤ºä¾‹æ•°æ®
    np.random.seed(42)
    dates = pd.date_range('2024-01-01', periods=200, freq='10min')
    
    # åŸºç¡€æ¨¡å¼ï¼šæ—¥å‘¨æœŸ + å™ªå£°
    base_pattern = 30 + 15 * np.sin(2 * np.pi * np.arange(200) / 144)
    noise = np.random.normal(0, 3, 200)
    cpu_data = base_pattern + noise
    
    # æ’å…¥ä¸åŒç±»å‹çš„å¼‚å¸¸
    cpu_data[50] = 80   # å°–å³°
    cpu_data[100] = 5   # ä¸‹é™
    cpu_data[150:155] += 20  # æ¼‚ç§»
    
    # åˆ›å»ºDataFrame
    df = pd.DataFrame({'cpu_usage': cpu_data}, index=dates)
    
    # åˆ›å»ºæ ¹å› åˆ†æå™¨
    analyzer = RootCauseAnalyzer()
    
    # åˆ†æå¼‚å¸¸ç‚¹
    anomaly_indices = [50, 100, 152]
    explanations = analyzer.batch_analyze_anomalies(df['cpu_usage'], anomaly_indices)
    
    # ç”ŸæˆæŠ¥å‘Š
    for i, explanation in enumerate(explanations):
        print(f"\nå¼‚å¸¸ç‚¹ {i+1}:")
        print(analyzer.generate_explanation_report(explanation))
        print("-" * 80)
    
    # å¯è§†åŒ–æœ€åä¸€ä¸ªå¼‚å¸¸çš„è§£é‡Š
    if explanations:
        analyzer.plot_anomaly_explanation(df['cpu_usage'], explanations[-1])
    
    print(f"\næ€»å…±åˆ†æäº† {len(explanations)} ä¸ªå¼‚å¸¸ç‚¹")
