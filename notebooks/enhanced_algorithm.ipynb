{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1261003e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CPUFeatureEngineer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 151\u001b[39m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m feature_df, algorithm_features\n\u001b[32m    150\u001b[39m \u001b[38;5;66;03m# ä¸ºCPUFeatureEngineerç±»æ·»åŠ è¿™ä¸ªæ–°æ–¹æ³•\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m \u001b[43mCPUFeatureEngineer\u001b[49m.create_enhanced_algorithm_features = create_enhanced_algorithm_features\n\u001b[32m    153\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâœ… å¢å¼ºç‰ˆç®—æ³•ç‰¹å¾å‡½æ•°å·²æ·»åŠ !\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'CPUFeatureEngineer' is not defined"
     ]
    }
   ],
   "source": [
    "# ============ æ”¹è¿›ç‰ˆç®—æ³•ç‰¹å¾å·¥ç¨‹ ============\n",
    "\n",
    "def create_enhanced_algorithm_features(self, df):\n",
    "    \"\"\"åˆ›å»ºå¢å¼ºç‰ˆç®—æ³•ç‰¹å¾ - æ·»åŠ æ›´å¤šä¸“ä¸šç®—æ³•\"\"\"\n",
    "    print(\"ğŸš€ åˆ›å»ºå¢å¼ºç‰ˆç®—æ³•ç‰¹å¾...\")\n",
    "    \n",
    "    feature_df = df.copy()\n",
    "    cpu_col = 'cpu_utilization'\n",
    "    algorithm_features = []\n",
    "    \n",
    "    # å‡†å¤‡æ•°æ®\n",
    "    cpu_data = feature_df[cpu_col].values.reshape(-1, 1)\n",
    "    cpu_series = feature_df[cpu_col]\n",
    "    \n",
    "    # ============ ç°æœ‰ç®—æ³• ============\n",
    "    \n",
    "    # 1. Isolation Forest\n",
    "    try:\n",
    "        iso_forest = IsolationForest(contamination=0.08, random_state=42)\n",
    "        iso_scores = iso_forest.fit_predict(cpu_data)\n",
    "        feature_df['iso_forest_anomaly'] = (iso_scores == -1).astype(int)\n",
    "        algorithm_features.append('iso_forest_anomaly')\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸ Isolation Forestå¤±è´¥: {e}\")\n",
    "        feature_df['iso_forest_anomaly'] = 0\n",
    "    \n",
    "    # 2. Local Outlier Factor\n",
    "    try:\n",
    "        lof = LocalOutlierFactor(n_neighbors=min(20, len(feature_df)-1), contamination=0.08)\n",
    "        lof_scores = lof.fit_predict(cpu_data)\n",
    "        feature_df['lof_anomaly'] = (lof_scores == -1).astype(int)\n",
    "        algorithm_features.append('lof_anomaly')\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸ LOFå¤±è´¥: {e}\")\n",
    "        feature_df['lof_anomaly'] = 0\n",
    "    \n",
    "    # 3. Z-score (æ»šåŠ¨çª—å£)\n",
    "    rolling_mean = cpu_series.rolling('60min').mean()\n",
    "    rolling_std = cpu_series.rolling('60min').std()\n",
    "    feature_df['cpu_zscore'] = (cpu_series - rolling_mean) / (rolling_std + 1e-8)\n",
    "    feature_df['zscore_anomaly'] = (np.abs(feature_df['cpu_zscore']) > 3).astype(int)\n",
    "    \n",
    "    # 4. IQRæ–¹æ³•\n",
    "    rolling_q75 = cpu_series.rolling('60min').quantile(0.75)\n",
    "    rolling_q25 = cpu_series.rolling('60min').quantile(0.25)\n",
    "    rolling_iqr = rolling_q75 - rolling_q25\n",
    "    iqr_upper = rolling_q75 + 1.5 * rolling_iqr\n",
    "    iqr_lower = rolling_q25 - 1.5 * rolling_iqr\n",
    "    feature_df['iqr_anomaly'] = ((cpu_series > iqr_upper) | (cpu_series < iqr_lower)).astype(int)\n",
    "    \n",
    "    algorithm_features.extend(['cpu_zscore', 'zscore_anomaly', 'iqr_anomaly'])\n",
    "    \n",
    "    # ============ æ–°å¢ä¸“ä¸šç®—æ³• ============\n",
    "    \n",
    "    # 5. EWMA (æŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡) - æ‚¨æåˆ°çš„ç®—æ³•\n",
    "    print(\"   ğŸ“ˆ æ·»åŠ EWMAå¼‚å¸¸æ£€æµ‹...\")\n",
    "    alpha = 0.3  # å¹³æ»‘å‚æ•°\n",
    "    ewma_mean = cpu_series.ewm(alpha=alpha).mean()\n",
    "    ewma_std = cpu_series.ewm(alpha=alpha).std()\n",
    "    feature_df['ewma_score'] = np.abs(cpu_series - ewma_mean) / (ewma_std + 1e-8)\n",
    "    feature_df['ewma_anomaly'] = (feature_df['ewma_score'] > 3).astype(int)\n",
    "    algorithm_features.extend(['ewma_score', 'ewma_anomaly'])\n",
    "    \n",
    "    # 6. LSTM Autoencoderæ¨¡æ‹Ÿ (ç®€åŒ–ç‰ˆ - ä½¿ç”¨æ»‘åŠ¨çª—å£é‡æ„è¯¯å·®)\n",
    "    print(\"   ğŸ§  æ·»åŠ é‡æ„è¯¯å·®æ£€æµ‹...\")\n",
    "    window_size = 10\n",
    "    reconstruction_errors = []\n",
    "    for i in range(len(cpu_series)):\n",
    "        if i < window_size:\n",
    "            reconstruction_errors.append(0)\n",
    "        else:\n",
    "            # ä½¿ç”¨å‰Nä¸ªç‚¹çš„å‡å€¼é¢„æµ‹å½“å‰ç‚¹\n",
    "            window_data = cpu_series.iloc[i-window_size:i]\n",
    "            predicted = window_data.mean()\n",
    "            error = abs(cpu_series.iloc[i] - predicted)\n",
    "            reconstruction_errors.append(error)\n",
    "    \n",
    "    feature_df['reconstruction_error'] = reconstruction_errors\n",
    "    # ä½¿ç”¨é‡æ„è¯¯å·®çš„95åˆ†ä½æ•°ä½œä¸ºé˜ˆå€¼\n",
    "    threshold = np.percentile(reconstruction_errors, 95)\n",
    "    feature_df['reconstruction_anomaly'] = (feature_df['reconstruction_error'] > threshold).astype(int)\n",
    "    algorithm_features.extend(['reconstruction_error', 'reconstruction_anomaly'])\n",
    "    \n",
    "    # 7. å˜åŒ–ç‚¹æ£€æµ‹ (Change Point Detection)\n",
    "    print(\"   ğŸ“Š æ·»åŠ å˜åŒ–ç‚¹æ£€æµ‹...\")\n",
    "    # è®¡ç®—æ»‘åŠ¨æ–¹å·®æ¯”å€¼æ¥æ£€æµ‹ç»“æ„æ€§å˜åŒ–\n",
    "    short_window = 15\n",
    "    long_window = 60\n",
    "    short_var = cpu_series.rolling(f'{short_window}min').var()\n",
    "    long_var = cpu_series.rolling(f'{long_window}min').var()\n",
    "    variance_ratio = short_var / (long_var + 1e-8)\n",
    "    feature_df['variance_ratio'] = variance_ratio\n",
    "    feature_df['changepoint_anomaly'] = (variance_ratio > 2.0).astype(int)  # çŸ­æœŸæ–¹å·®æ˜æ˜¾é«˜äºé•¿æœŸ\n",
    "    algorithm_features.extend(['variance_ratio', 'changepoint_anomaly'])\n",
    "    \n",
    "    # 8. å­£èŠ‚æ€§åˆ†è§£å¼‚å¸¸æ£€æµ‹\n",
    "    print(\"   ğŸ”„ æ·»åŠ å­£èŠ‚æ€§å¼‚å¸¸æ£€æµ‹...\")\n",
    "    # ç®€åŒ–çš„å­£èŠ‚æ€§åˆ†è§£ï¼šæ¯å°æ—¶æ¨¡å¼\n",
    "    hourly_pattern = cpu_series.groupby(cpu_series.index.hour).transform('mean')\n",
    "    seasonal_residual = cpu_series - hourly_pattern\n",
    "    seasonal_std = seasonal_residual.rolling('120min').std()\n",
    "    feature_df['seasonal_residual'] = seasonal_residual\n",
    "    feature_df['seasonal_anomaly'] = (np.abs(seasonal_residual) > 2 * seasonal_std).astype(int)\n",
    "    algorithm_features.extend(['seasonal_residual', 'seasonal_anomaly'])\n",
    "    \n",
    "    # ============ å¢å¼ºçš„é›†æˆç‰¹å¾ ============\n",
    "    \n",
    "    # æ‰©å±•ç®—æ³•åˆ—è¡¨\n",
    "    enhanced_algorithm_cols = [\n",
    "        'iso_forest_anomaly', 'lof_anomaly', 'zscore_anomaly', 'iqr_anomaly',\n",
    "        'ewma_anomaly', 'reconstruction_anomaly', 'changepoint_anomaly', 'seasonal_anomaly'\n",
    "    ]\n",
    "    \n",
    "    # é‡æ–°è®¡ç®—é›†æˆç‰¹å¾\n",
    "    feature_df['algorithm_consensus'] = feature_df[enhanced_algorithm_cols].sum(axis=1)\n",
    "    feature_df['algorithm_agreement'] = (feature_df['algorithm_consensus'] >= 3).astype(int)  # è‡³å°‘3ä¸ªç®—æ³•ä¸€è‡´\n",
    "    feature_df['strong_consensus'] = (feature_df['algorithm_consensus'] >= 5).astype(int)     # å¼ºä¸€è‡´æ€§ï¼š5ä¸ªä»¥ä¸Š\n",
    "    \n",
    "    # åŠ æƒä¸€è‡´æ€§ï¼ˆç»™ä¸åŒç®—æ³•ä¸åŒæƒé‡ï¼‰\n",
    "    weights = {\n",
    "        'iso_forest_anomaly': 0.15,     # æœºå™¨å­¦ä¹ ç®—æ³•æƒé‡é«˜\n",
    "        'lof_anomaly': 0.15,\n",
    "        'ewma_anomaly': 0.15,           # æ—¶åºä¸“ç”¨ç®—æ³•æƒé‡é«˜\n",
    "        'seasonal_anomaly': 0.15,\n",
    "        'zscore_anomaly': 0.1,          # ç»Ÿè®¡ç®—æ³•æƒé‡ä¸­ç­‰\n",
    "        'iqr_anomaly': 0.1,\n",
    "        'reconstruction_anomaly': 0.1,\n",
    "        'changepoint_anomaly': 0.1\n",
    "    }\n",
    "    \n",
    "    weighted_score = sum(feature_df[col] * weight for col, weight in weights.items())\n",
    "    feature_df['weighted_consensus'] = weighted_score\n",
    "    feature_df['weighted_anomaly'] = (weighted_score > 0.3).astype(int)  # åŠ æƒé˜ˆå€¼\n",
    "    \n",
    "    algorithm_features.extend([\n",
    "        'algorithm_consensus', 'algorithm_agreement', 'strong_consensus',\n",
    "        'weighted_consensus', 'weighted_anomaly'\n",
    "    ])\n",
    "    \n",
    "    print(f\"   âœ… å¢å¼ºç®—æ³•ç‰¹å¾: {len(algorithm_features)} ä¸ª\")\n",
    "    print(f\"   ğŸ“Š ç®—æ³•è¦†ç›–:\")\n",
    "    print(f\"      - æœºå™¨å­¦ä¹ : Isolation Forest, LOF\")\n",
    "    print(f\"      - æ—¶åºä¸“ç”¨: EWMA, å­£èŠ‚æ€§åˆ†è§£\")\n",
    "    print(f\"      - ç»Ÿè®¡æ–¹æ³•: Z-score, IQR\")\n",
    "    print(f\"      - ç»“æ„æ£€æµ‹: å˜åŒ–ç‚¹, é‡æ„è¯¯å·®\")\n",
    "    print(f\"      - é›†æˆæ–¹æ³•: æŠ•ç¥¨, åŠ æƒä¸€è‡´æ€§\")\n",
    "    \n",
    "    return feature_df, algorithm_features\n",
    "\n",
    "# ä¸ºCPUFeatureEngineerç±»æ·»åŠ è¿™ä¸ªæ–°æ–¹æ³•\n",
    "CPUFeatureEngineer.create_enhanced_algorithm_features = create_enhanced_algorithm_features\n",
    "\n",
    "print(\"âœ… å¢å¼ºç‰ˆç®—æ³•ç‰¹å¾å‡½æ•°å·²æ·»åŠ !\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
