"""
Seeker å¼‚å¸¸æ£€æµ‹é›†æˆæ¡†æ¶
ç»Ÿä¸€ç®¡ç†å’Œè°ƒç”¨æ‰€æœ‰å¼‚å¸¸æ£€æµ‹æ–¹æ³•çš„ä¸»æ§åˆ¶å™¨
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import json
import os
from datetime import datetime
from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass, asdict
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥å„ç§æ£€æµ‹æ–¹æ³•
from point_anomalies.main import (
    zscore_detection, iqr_detection, ewma_detection,
    isolation_forest_detection, lof_detection
)

# å¯¼å…¥æ–°å¼€å‘çš„é«˜çº§æ–¹æ³•
try:
    from deep_learning_methods.lstm_autoencoder import lstm_autoencoder_detection
    DEEP_LEARNING_AVAILABLE = True
except ImportError:
    DEEP_LEARNING_AVAILABLE = False
    print("è­¦å‘Š: æ·±åº¦å­¦ä¹ æ–¹æ³•ä¸å¯ç”¨ï¼Œè¯·å®‰è£…tensorflow")

try:
    from seasonal_methods.seasonal_anomaly_detection import seasonal_anomaly_detection
    SEASONAL_AVAILABLE = True
except ImportError:
    SEASONAL_AVAILABLE = False
    print("è­¦å‘Š: å­£èŠ‚æ€§æ£€æµ‹æ–¹æ³•ä¸å¯ç”¨ï¼Œè¯·å®‰è£…statsmodels")

try:
    from streaming_methods.streaming_anomaly_detection import StreamingAnomalyDetector
    STREAMING_AVAILABLE = True
except ImportError:
    STREAMING_AVAILABLE = False

try:
    from root_cause_analysis.anomaly_explanation import RootCauseAnalyzer
    ROOT_CAUSE_AVAILABLE = True
except ImportError:
    ROOT_CAUSE_AVAILABLE = False

try:
    from multimetric_methods.multimetric_anomaly_detection import MultiMetricAnomalyDetector
    MULTIMETRIC_AVAILABLE = True
except ImportError:
    MULTIMETRIC_AVAILABLE = False

@dataclass
class DetectionResult:
    """æ£€æµ‹ç»“æœç»Ÿä¸€æ•°æ®ç»“æ„"""
    method: str
    anomalies: np.ndarray  # å¸ƒå°”æ•°ç»„ï¼Œæ ‡è®°å¼‚å¸¸ä½ç½®
    scores: Optional[np.ndarray] = None  # å¼‚å¸¸åˆ†æ•°
    parameters: Optional[Dict] = None
    execution_time: Optional[float] = None
    metadata: Optional[Dict] = None

@dataclass
class SeekerConfig:
    """Seekeré…ç½®ç±»"""
    # åŸºç¡€æ–¹æ³•å‚æ•°
    zscore_threshold: float = 3.0
    iqr_k: float = 1.5
    ewma_span: int = 15
    ewma_threshold: float = 2.0
    
    # æœºå™¨å­¦ä¹ æ–¹æ³•å‚æ•°
    isolation_forest_contamination: float = 0.02
    lof_n_neighbors: int = 20
    lof_contamination: float = 0.02
    
    # æ·±åº¦å­¦ä¹ å‚æ•°
    lstm_sequence_length: int = 30
    lstm_encoding_dim: int = 10
    lstm_epochs: int = 50
    
    # å­£èŠ‚æ€§æ£€æµ‹å‚æ•°
    seasonal_period: int = 144  # æ—¥å‘¨æœŸï¼Œ10åˆ†é’Ÿé—´éš”
    seasonal_weights: Dict[str, float] = None
    
    # è¾“å‡ºæ§åˆ¶
    enable_visualization: bool = True
    enable_reports: bool = True
    output_dir: str = "results"
    
    def __post_init__(self):
        if self.seasonal_weights is None:
            self.seasonal_weights = {'trend': 0.3, 'seasonal': 0.3, 'residual': 0.4}

class SeekerAnomalyDetector:
    """Seeker ä¸»å¼‚å¸¸æ£€æµ‹å™¨"""
    
    def __init__(self, config: Optional[SeekerConfig] = None):
        """
        Args:
            config: é…ç½®å¯¹è±¡ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        """
        self.config = config or SeekerConfig()
        self.results = {}
        self.data = None
        self.root_cause_analyzer = None
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        Path(self.config.output_dir).mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–ç»„ä»¶
        if ROOT_CAUSE_AVAILABLE:
            self.root_cause_analyzer = RootCauseAnalyzer()
    
    def detect_single_metric(self, data: Union[pd.Series, np.ndarray], 
                           methods: List[str] = None,
                           enable_explanation: bool = False) -> Dict[str, DetectionResult]:
        """
        å•æŒ‡æ ‡å¼‚å¸¸æ£€æµ‹
        
        Args:
            data: æ—¶é—´åºåˆ—æ•°æ®
            methods: ä½¿ç”¨çš„æ£€æµ‹æ–¹æ³•åˆ—è¡¨
            enable_explanation: æ˜¯å¦å¯ç”¨å¼‚å¸¸è§£é‡Š
        """
        if methods is None:
            methods = ['zscore', 'iqr', 'ewma', 'isolation_forest', 'lof']
        
        # æ•°æ®é¢„å¤„ç†
        if isinstance(data, pd.Series):
            self.data = data
            values = data.values
        else:
            values = data
            self.data = pd.Series(values)
        
        results = {}
        
        # åŸºç¡€ç»Ÿè®¡æ–¹æ³•
        if 'zscore' in methods:
            start_time = datetime.now()
            anomalies = zscore_detection(values, self.config.zscore_threshold)
            execution_time = (datetime.now() - start_time).total_seconds()
            
            results['zscore'] = DetectionResult(
                method='zscore',
                anomalies=anomalies,
                parameters={'threshold': self.config.zscore_threshold},
                execution_time=execution_time
            )
        
        if 'iqr' in methods:
            start_time = datetime.now()
            anomalies = iqr_detection(values, self.config.iqr_k)
            execution_time = (datetime.now() - start_time).total_seconds()
            
            results['iqr'] = DetectionResult(
                method='iqr',
                anomalies=anomalies,
                parameters={'k': self.config.iqr_k},
                execution_time=execution_time
            )
        
        if 'ewma' in methods:
            start_time = datetime.now()
            anomalies = ewma_detection(values, self.config.ewma_span, self.config.ewma_threshold)
            execution_time = (datetime.now() - start_time).total_seconds()
            
            results['ewma'] = DetectionResult(
                method='ewma',
                anomalies=anomalies,
                parameters={'span': self.config.ewma_span, 'threshold': self.config.ewma_threshold},
                execution_time=execution_time
            )
        
        # æœºå™¨å­¦ä¹ æ–¹æ³•
        if 'isolation_forest' in methods:
            start_time = datetime.now()
            anomalies = isolation_forest_detection(values, self.config.isolation_forest_contamination)
            execution_time = (datetime.now() - start_time).total_seconds()
            
            results['isolation_forest'] = DetectionResult(
                method='isolation_forest',
                anomalies=anomalies,
                parameters={'contamination': self.config.isolation_forest_contamination},
                execution_time=execution_time
            )
        
        if 'lof' in methods:
            start_time = datetime.now()
            anomalies = lof_detection(values, self.config.lof_n_neighbors, self.config.lof_contamination)
            execution_time = (datetime.now() - start_time).total_seconds()
            
            results['lof'] = DetectionResult(
                method='lof',
                anomalies=anomalies,
                parameters={'n_neighbors': self.config.lof_n_neighbors, 'contamination': self.config.lof_contamination},
                execution_time=execution_time
            )
        
        # æ·±åº¦å­¦ä¹ æ–¹æ³•
        if 'lstm_autoencoder' in methods and DEEP_LEARNING_AVAILABLE:
            start_time = datetime.now()
            try:
                anomalies, scores, detector = lstm_autoencoder_detection(
                    values,
                    sequence_length=self.config.lstm_sequence_length,
                    encoding_dim=self.config.lstm_encoding_dim,
                    epochs=self.config.lstm_epochs
                )
                execution_time = (datetime.now() - start_time).total_seconds()
                
                results['lstm_autoencoder'] = DetectionResult(
                    method='lstm_autoencoder',
                    anomalies=anomalies,
                    scores=scores,
                    parameters={
                        'sequence_length': self.config.lstm_sequence_length,
                        'encoding_dim': self.config.lstm_encoding_dim,
                        'epochs': self.config.lstm_epochs
                    },
                    execution_time=execution_time
                )
            except Exception as e:
                print(f"LSTMè‡ªç¼–ç å™¨æ£€æµ‹å¤±è´¥: {e}")
        
        # å­£èŠ‚æ€§å¼‚å¸¸æ£€æµ‹
        if 'seasonal' in methods and SEASONAL_AVAILABLE:
            start_time = datetime.now()
            try:
                seasonal_results, detector = seasonal_anomaly_detection(
                    values,
                    period=self.config.seasonal_period,
                    weights=self.config.seasonal_weights
                )
                execution_time = (datetime.now() - start_time).total_seconds()
                
                results['seasonal'] = DetectionResult(
                    method='seasonal',
                    anomalies=seasonal_results['final_anomalies'],
                    scores=seasonal_results['anomaly_score'],
                    parameters={
                        'period': self.config.seasonal_period,
                        'weights': self.config.seasonal_weights
                    },
                    execution_time=execution_time,
                    metadata=seasonal_results
                )
            except Exception as e:
                print(f"å­£èŠ‚æ€§æ£€æµ‹å¤±è´¥: {e}")
        
        # å¼‚å¸¸è§£é‡Šï¼ˆå¦‚æœå¯ç”¨ä¸”æœ‰å¼‚å¸¸ï¼‰
        if enable_explanation and ROOT_CAUSE_AVAILABLE:
            self._add_explanations(results)
        
        self.results = results
        return results
    
    def detect_multi_metric(self, data: pd.DataFrame, 
                          contamination: float = None) -> Dict[str, Any]:
        """
        å¤šæŒ‡æ ‡å¼‚å¸¸æ£€æµ‹
        
        Args:
            data: å¤šæŒ‡æ ‡æ•°æ®DataFrame
            contamination: é¢„æœŸå¼‚å¸¸æ¯”ä¾‹
        """
        if not MULTIMETRIC_AVAILABLE:
            raise ImportError("å¤šæŒ‡æ ‡æ£€æµ‹ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥ä¾èµ–")
        
        contamination = contamination or self.config.isolation_forest_contamination
        
        start_time = datetime.now()
        detector = MultiMetricAnomalyDetector(contamination=contamination)
        anomalies = detector.detect_anomalies(data)
        execution_time = (datetime.now() - start_time).total_seconds()
        
        # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
        anomaly_mask = np.zeros(len(data), dtype=bool)
        anomaly_scores = np.zeros(len(data))
        
        for anomaly in anomalies:
            if isinstance(anomaly.timestamp, (int, np.integer)):
                idx = anomaly.timestamp
            else:
                try:
                    idx = data.index.get_loc(anomaly.timestamp)
                except:
                    continue
            
            if 0 <= idx < len(data):
                anomaly_mask[idx] = True
                anomaly_scores[idx] = anomaly.anomaly_score
        
        return {
            'detector': detector,
            'anomalies': anomalies,
            'anomaly_mask': anomaly_mask,
            'anomaly_scores': anomaly_scores,
            'execution_time': execution_time
        }
    
    def create_streaming_detector(self, window_size: int = 100, 
                                methods: List[str] = None) -> 'StreamingAnomalyDetector':
        """
        åˆ›å»ºæµå¼æ£€æµ‹å™¨
        
        Args:
            window_size: æ»‘åŠ¨çª—å£å¤§å°
            methods: ä½¿ç”¨çš„æ£€æµ‹æ–¹æ³•
        """
        if not STREAMING_AVAILABLE:
            raise ImportError("æµå¼æ£€æµ‹ä¸å¯ç”¨")
        
        if methods is None:
            methods = ['zscore', 'ewma', 'isolation_forest']
        
        return StreamingAnomalyDetector(window_size=window_size, methods=methods)
    
    def _add_explanations(self, results: Dict[str, DetectionResult]):
        """ä¸ºæ£€æµ‹ç»“æœæ·»åŠ è§£é‡Š"""
        if not self.root_cause_analyzer or self.data is None:
            return
        
        # æ”¶é›†æ‰€æœ‰å¼‚å¸¸ç‚¹
        all_anomaly_indices = set()
        for result in results.values():
            anomaly_indices = np.where(result.anomalies)[0]
            all_anomaly_indices.update(anomaly_indices)
        
        # ä¸ºä¸»è¦å¼‚å¸¸ç‚¹ç”Ÿæˆè§£é‡Š
        explanations = {}
        for idx in list(all_anomaly_indices)[:10]:  # é™åˆ¶è§£é‡Šæ•°é‡
            try:
                explanation = self.root_cause_analyzer.analyze_anomaly(self.data, idx)
                explanations[idx] = explanation
            except Exception as e:
                print(f"è§£é‡Šç”Ÿæˆå¤±è´¥ (ç´¢å¼• {idx}): {e}")
        
        # å°†è§£é‡Šæ·»åŠ åˆ°ç»“æœä¸­
        for result in results.values():
            if result.metadata is None:
                result.metadata = {}
            result.metadata['explanations'] = explanations
    
    def ensemble_detection(self, results: Dict[str, DetectionResult], 
                          voting_threshold: float = 0.3) -> DetectionResult:
        """
        é›†æˆå¤šç§æ–¹æ³•çš„æ£€æµ‹ç»“æœ
        
        Args:
            results: å„æ–¹æ³•çš„æ£€æµ‹ç»“æœ
            voting_threshold: æŠ•ç¥¨é˜ˆå€¼
        """
        if not results:
            raise ValueError("æ²¡æœ‰æ£€æµ‹ç»“æœå¯ç”¨äºé›†æˆ")
        
        # è·å–æ•°æ®é•¿åº¦
        data_length = len(next(iter(results.values())).anomalies)
        
        # æŠ•ç¥¨é›†æˆ
        vote_matrix = np.zeros((len(results), data_length))
        method_names = []
        
        for i, (method, result) in enumerate(results.items()):
            vote_matrix[i] = result.anomalies.astype(int)
            method_names.append(method)
        
        # è®¡ç®—æŠ•ç¥¨åˆ†æ•°
        vote_scores = np.mean(vote_matrix, axis=0)
        ensemble_anomalies = vote_scores >= voting_threshold
        
        return DetectionResult(
            method='ensemble',
            anomalies=ensemble_anomalies,
            scores=vote_scores,
            parameters={'voting_threshold': voting_threshold, 'methods': method_names},
            metadata={'vote_matrix': vote_matrix, 'method_names': method_names}
        )
    
    def plot_comprehensive_analysis(self, results: Dict[str, DetectionResult], 
                                  title: str = "Seeker Comprehensive Anomaly Analysis"):
        """ç»˜åˆ¶ç»¼åˆåˆ†æå›¾"""
        if not self.config.enable_visualization:
            return
        
        n_methods = len(results)
        fig, axes = plt.subplots(n_methods + 1, 1, figsize=(15, 4 * (n_methods + 1)))
        
        if n_methods == 1:
            axes = [axes]
        
        # åŸå§‹æ•°æ®
        data_values = self.data.values if self.data is not None else np.arange(len(next(iter(results.values())).anomalies))
        
        # ä¸ºæ¯ç§æ–¹æ³•ç»˜åˆ¶å­å›¾
        for i, (method, result) in enumerate(results.items()):
            ax = axes[i]
            
            # ç»˜åˆ¶æ•°æ®
            ax.plot(data_values, alpha=0.7, label='Data', linewidth=1)
            
            # æ ‡è®°å¼‚å¸¸ç‚¹
            anomaly_indices = np.where(result.anomalies)[0]
            if len(anomaly_indices) > 0:
                ax.scatter(anomaly_indices, data_values[anomaly_indices],
                          color='red', s=50, alpha=0.8, label='Anomalies')
            
            # æ·»åŠ åˆ†æ•°ï¼ˆå¦‚æœæœ‰ï¼‰
            if result.scores is not None:
                ax2 = ax.twinx()
                ax2.plot(result.scores, color='orange', alpha=0.5, label='Scores')
                ax2.set_ylabel('Anomaly Score', color='orange')
            
            ax.set_title(f'{method.upper()} Detection (Count: {result.anomalies.sum()})')
            ax.set_ylabel('Value')
            ax.legend(loc='upper left')
            ax.grid(True, alpha=0.3)
        
        # é›†æˆç»“æœ
        if len(results) > 1:
            ensemble_result = self.ensemble_detection(results)
            ax = axes[-1]
            
            ax.plot(data_values, alpha=0.7, label='Data', linewidth=1)
            
            anomaly_indices = np.where(ensemble_result.anomalies)[0]
            if len(anomaly_indices) > 0:
                ax.scatter(anomaly_indices, data_values[anomaly_indices],
                          color='purple', s=70, alpha=0.8, label='Ensemble Anomalies')
            
            # æŠ•ç¥¨åˆ†æ•°
            ax2 = ax.twinx()
            ax2.plot(ensemble_result.scores, color='green', alpha=0.6, label='Vote Scores')
            ax2.set_ylabel('Vote Score', color='green')
            ax2.axhline(y=ensemble_result.parameters['voting_threshold'], 
                       color='green', linestyle='--', alpha=0.5, label='Threshold')
            
            ax.set_title(f'Ensemble Detection (Count: {ensemble_result.anomalies.sum()})')
            ax.set_xlabel('Time Index')
            ax.set_ylabel('Value')
            ax.legend(loc='upper left')
            ax2.legend(loc='upper right')
            ax.grid(True, alpha=0.3)
        
        plt.suptitle(title, fontsize=16)
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        save_path = os.path.join(self.config.output_dir, 'comprehensive_analysis.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ç»¼åˆåˆ†æå›¾å·²ä¿å­˜è‡³: {save_path}")
        
        if self.config.enable_visualization:
            plt.show()
    
    def generate_report(self, results: Dict[str, DetectionResult], 
                       output_format: str = 'both') -> Dict[str, str]:
        """
        ç”Ÿæˆæ£€æµ‹æŠ¥å‘Š
        
        Args:
            results: æ£€æµ‹ç»“æœ
            output_format: è¾“å‡ºæ ¼å¼ ('json', 'text', 'both')
        """
        if not self.config.enable_reports:
            return {}
        
        # ç”ŸæˆæŠ¥å‘Šå†…å®¹
        report_data = {
            'timestamp': datetime.now().isoformat(),
            'data_length': len(next(iter(results.values())).anomalies),
            'methods_used': list(results.keys()),
            'summary': {},
            'performance': {},
            'detailed_results': {}
        }
        
        # æ±‡æ€»ç»Ÿè®¡
        for method, result in results.items():
            anomaly_count = result.anomalies.sum()
            anomaly_rate = anomaly_count / len(result.anomalies) * 100
            
            report_data['summary'][method] = {
                'anomaly_count': int(anomaly_count),
                'anomaly_rate': f"{anomaly_rate:.2f}%",
                'parameters': result.parameters
            }
            
            if result.execution_time:
                report_data['performance'][method] = f"{result.execution_time:.4f}s"
            
            report_data['detailed_results'][method] = {
                'anomaly_indices': np.where(result.anomalies)[0].tolist(),
                'has_scores': result.scores is not None,
                'metadata_available': result.metadata is not None
            }
        
        # é›†æˆç»“æœ
        if len(results) > 1:
            ensemble_result = self.ensemble_detection(results)
            ensemble_count = ensemble_result.anomalies.sum()
            ensemble_rate = ensemble_count / len(ensemble_result.anomalies) * 100
            
            report_data['ensemble'] = {
                'anomaly_count': int(ensemble_count),
                'anomaly_rate': f"{ensemble_rate:.2f}%",
                'voting_threshold': ensemble_result.parameters['voting_threshold']
            }
        
        # ä¿å­˜æŠ¥å‘Š
        reports = {}
        
        if output_format in ['json', 'both']:
            json_path = os.path.join(self.config.output_dir, 'detection_report.json')
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, indent=2, ensure_ascii=False)
            reports['json'] = json_path
        
        if output_format in ['text', 'both']:
            text_report = self._generate_text_report(report_data)
            text_path = os.path.join(self.config.output_dir, 'detection_report.txt')
            with open(text_path, 'w', encoding='utf-8') as f:
                f.write(text_report)
            reports['text'] = text_path
        
        return reports
    
    def _generate_text_report(self, report_data: Dict) -> str:
        """ç”Ÿæˆæ–‡æœ¬æ ¼å¼æŠ¥å‘Š"""
        lines = []
        
        lines.append("ğŸ” SEEKER å¼‚å¸¸æ£€æµ‹æŠ¥å‘Š")
        lines.append("=" * 60)
        lines.append(f"ç”Ÿæˆæ—¶é—´: {report_data['timestamp']}")
        lines.append(f"æ•°æ®é•¿åº¦: {report_data['data_length']}")
        lines.append(f"ä½¿ç”¨æ–¹æ³•: {', '.join(report_data['methods_used'])}")
        lines.append("")
        
        lines.append("ğŸ“Š æ£€æµ‹æ±‡æ€»:")
        for method, summary in report_data['summary'].items():
            lines.append(f"  {method.upper()}:")
            lines.append(f"    å¼‚å¸¸æ•°é‡: {summary['anomaly_count']}")
            lines.append(f"    å¼‚å¸¸ç‡: {summary['anomaly_rate']}")
            lines.append(f"    å‚æ•°: {summary['parameters']}")
        
        if 'ensemble' in report_data:
            lines.append(f"  ENSEMBLE:")
            lines.append(f"    å¼‚å¸¸æ•°é‡: {report_data['ensemble']['anomaly_count']}")
            lines.append(f"    å¼‚å¸¸ç‡: {report_data['ensemble']['anomaly_rate']}")
            lines.append(f"    æŠ•ç¥¨é˜ˆå€¼: {report_data['ensemble']['voting_threshold']}")
        
        lines.append("")
        
        if report_data['performance']:
            lines.append("âš¡ æ€§èƒ½ç»Ÿè®¡:")
            for method, time_str in report_data['performance'].items():
                lines.append(f"  {method}: {time_str}")
            lines.append("")
        
        lines.append("ğŸ¯ å»ºè®®:")
        # æ ¹æ®ç»“æœç”Ÿæˆå»ºè®®
        anomaly_counts = [summary['anomaly_count'] for summary in report_data['summary'].values()]
        if max(anomaly_counts) == 0:
            lines.append("  - æœªæ£€æµ‹åˆ°å¼‚å¸¸ï¼Œæ•°æ®çœ‹èµ·æ¥æ­£å¸¸")
        elif max(anomaly_counts) > len(report_data['methods_used']) * 5:
            lines.append("  - æ£€æµ‹åˆ°å¤§é‡å¼‚å¸¸ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®è´¨é‡æˆ–è°ƒæ•´å‚æ•°")
        else:
            lines.append("  - æ£€æµ‹ç»“æœåˆç†ï¼Œå»ºè®®è¿›ä¸€æ­¥åˆ†æå¼‚å¸¸åŸå› ")
        
        return "\n".join(lines)

# ä¾¿æ·å‡½æ•°
def quick_detect(data: Union[pd.Series, np.ndarray], 
                methods: List[str] = None,
                config: SeekerConfig = None,
                enable_visualization: bool = True,
                enable_reports: bool = True) -> Dict[str, DetectionResult]:
    """
    å¿«é€Ÿå¼‚å¸¸æ£€æµ‹å‡½æ•°
    
    Args:
        data: æ—¶é—´åºåˆ—æ•°æ®
        methods: æ£€æµ‹æ–¹æ³•åˆ—è¡¨
        config: é…ç½®å¯¹è±¡
        enable_visualization: æ˜¯å¦æ˜¾ç¤ºå¯è§†åŒ–
        enable_reports: æ˜¯å¦ç”ŸæˆæŠ¥å‘Š
    """
    if config is None:
        config = SeekerConfig()
    
    config.enable_visualization = enable_visualization
    config.enable_reports = enable_reports
    
    detector = SeekerAnomalyDetector(config)
    results = detector.detect_single_metric(data, methods)
    
    if enable_visualization:
        detector.plot_comprehensive_analysis(results)
    
    if enable_reports:
        reports = detector.generate_report(results)
        print(f"ğŸ“„ æŠ¥å‘Šå·²ç”Ÿæˆ: {reports}")
    
    return results

# ç¤ºä¾‹ç”¨æ³•
if __name__ == "__main__":
    # ç”Ÿæˆç¤ºä¾‹æ•°æ®
    np.random.seed(42)
    dates = pd.date_range('2024-01-01', periods=200, freq='10min')
    
    # åˆ›å»ºæ¨¡æ‹ŸCPUä½¿ç”¨ç‡æ•°æ®
    base_pattern = 30 + 15 * np.sin(2 * np.pi * np.arange(200) / 144)
    noise = np.random.normal(0, 3, 200)
    cpu_data = base_pattern + noise
    
    # æ’å…¥å¼‚å¸¸
    cpu_data[[50, 100, 150]] = [85, 5, 90]
    
    # åˆ›å»ºæ—¶é—´åºåˆ—
    ts_data = pd.Series(cpu_data, index=dates)
    
    print("ğŸš€ å¼€å§‹ Seeker ç»¼åˆå¼‚å¸¸æ£€æµ‹æ¼”ç¤º")
    print("=" * 60)
    
    # ä½¿ç”¨å¿«é€Ÿæ£€æµ‹å‡½æ•°
    methods_to_test = ['zscore', 'iqr', 'ewma', 'isolation_forest', 'lof']
    
    # æ·»åŠ é«˜çº§æ–¹æ³•ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if DEEP_LEARNING_AVAILABLE:
        methods_to_test.append('lstm_autoencoder')
    if SEASONAL_AVAILABLE:
        methods_to_test.append('seasonal')
    
    # è‡ªå®šä¹‰é…ç½®
    config = SeekerConfig(
        zscore_threshold=2.5,
        iqr_k=2.0,
        ewma_span=20,
        enable_visualization=True,
        enable_reports=True,
        output_dir="seeker_results"
    )
    
    # æ‰§è¡Œæ£€æµ‹
    results = quick_detect(
        data=ts_data,
        methods=methods_to_test,
        config=config,
        enable_visualization=True,
        enable_reports=True
    )
    
    print(f"\nâœ… æ£€æµ‹å®Œæˆï¼ä½¿ç”¨äº† {len(results)} ç§æ–¹æ³•")
    print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {config.output_dir}")
    
    # æ‰“å°ç®€è¦ç»Ÿè®¡
    for method, result in results.items():
        count = result.anomalies.sum()
        rate = count / len(result.anomalies) * 100
        time_str = f" ({result.execution_time:.3f}s)" if result.execution_time else ""
        print(f"  {method}: {count} ä¸ªå¼‚å¸¸ ({rate:.1f}%){time_str}")
