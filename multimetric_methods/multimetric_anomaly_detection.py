"""
å¤šç»´åº¦ä¸å¤šæŒ‡æ ‡å¼‚å¸¸æ£€æµ‹
å¤„ç†å¤šä¸ªç›¸å…³æ—¶é—´åºåˆ—çš„è”åˆå¼‚å¸¸æ£€æµ‹
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
from sklearn.covariance import EmpiricalCovariance, MinCovDet
from sklearn.cluster import DBSCAN
import seaborn as sns
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
import warnings
warnings.filterwarnings('ignore')

@dataclass
class MultiMetricAnomaly:
    """å¤šæŒ‡æ ‡å¼‚å¸¸ç»“æœ"""
    timestamp: pd.Timestamp
    anomaly_score: float
    affected_metrics: List[str]
    anomaly_type: str  # 'correlation', 'outlier', 'pattern', 'combined'
    individual_scores: Dict[str, float]
    correlation_changes: Dict[str, float]

class MultiMetricAnomalyDetector:
    """å¤šæŒ‡æ ‡å¼‚å¸¸æ£€æµ‹å™¨"""
    
    def __init__(self, contamination=0.02):
        """
        Args:
            contamination: é¢„æœŸå¼‚å¸¸æ¯”ä¾‹
        """
        self.contamination = contamination
        self.scaler = StandardScaler()
        self.pca = PCA()
        self.baseline_correlations = None
        self.isolation_forest = None
        self.robust_cov = None
        self.fitted = False
        
    def fit(self, data: pd.DataFrame):
        """
        è®­ç»ƒå¤šæŒ‡æ ‡æ£€æµ‹æ¨¡å‹
        
        Args:
            data: å¤šæŒ‡æ ‡æ•°æ®DataFrameï¼Œæ¯åˆ—ä¸ºä¸€ä¸ªæŒ‡æ ‡
        """
        # æ•°æ®é¢„å¤„ç†
        scaled_data = self.scaler.fit_transform(data.fillna(method='ffill').fillna(method='bfill'))
        
        # PCAé™ç»´åˆ†æ
        self.pca.fit(scaled_data)
        
        # è®­ç»ƒIsolation Forest
        self.isolation_forest = IsolationForest(
            contamination=self.contamination,
            random_state=42
        )
        self.isolation_forest.fit(scaled_data)
        
        # è®¡ç®—åŸºçº¿ç›¸å…³æ€§
        self.baseline_correlations = data.corr()
        
        # ç¨³å¥åæ–¹å·®ä¼°è®¡
        self.robust_cov = MinCovDet(contamination=self.contamination)
        self.robust_cov.fit(scaled_data)
        
        self.fitted = True
    
    def detect_correlation_anomalies(self, data: pd.DataFrame, window_size: int = 20) -> np.ndarray:
        """æ£€æµ‹ç›¸å…³æ€§å¼‚å¸¸"""
        if self.baseline_correlations is None:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨fitæ–¹æ³•")
        
        correlation_anomalies = np.zeros(len(data), dtype=bool)
        correlation_changes = []
        
        for i in range(window_size, len(data)):
            # è®¡ç®—æ»‘åŠ¨çª—å£ç›¸å…³æ€§
            window_data = data.iloc[i-window_size:i]
            window_corr = window_data.corr()
            
            # è®¡ç®—ä¸åŸºçº¿ç›¸å…³æ€§çš„åå·®
            corr_diff = np.abs(window_corr - self.baseline_correlations)
            avg_corr_change = np.mean(corr_diff.values[np.triu_indices_from(corr_diff, k=1)])
            correlation_changes.append(avg_corr_change)
            
            # å¼‚å¸¸åˆ¤å®šï¼ˆä½¿ç”¨è‡ªé€‚åº”é˜ˆå€¼ï¼‰
            if i >= window_size * 2:
                recent_changes = correlation_changes[-window_size:]
                threshold = np.mean(recent_changes) + 2 * np.std(recent_changes)
                correlation_anomalies[i] = avg_corr_change > threshold
        
        return correlation_anomalies
    
    def detect_multivariate_outliers(self, data: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """æ£€æµ‹å¤šå˜é‡ç¦»ç¾¤ç‚¹"""
        if not self.fitted:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨fitæ–¹æ³•")
        
        # æ•°æ®æ ‡å‡†åŒ–
        scaled_data = self.scaler.transform(data.fillna(method='ffill').fillna(method='bfill'))
        
        # Isolation Forestæ£€æµ‹
        isolation_scores = self.isolation_forest.decision_function(scaled_data)
        isolation_anomalies = self.isolation_forest.predict(scaled_data) == -1
        
        # é©¬æ°è·ç¦»æ£€æµ‹
        try:
            mahalanobis_distances = self.robust_cov.mahalanobis(scaled_data)
            # ä½¿ç”¨å¡æ–¹åˆ†å¸ƒé˜ˆå€¼
            threshold = np.percentile(mahalanobis_distances, (1 - self.contamination) * 100)
            mahalanobis_anomalies = mahalanobis_distances > threshold
        except:
            mahalanobis_anomalies = np.zeros(len(data), dtype=bool)
        
        # ç»¼åˆåˆ¤å®š
        combined_anomalies = isolation_anomalies | mahalanobis_anomalies
        
        return combined_anomalies, isolation_scores
    
    def detect_pattern_anomalies(self, data: pd.DataFrame) -> np.ndarray:
        """æ£€æµ‹æ¨¡å¼å¼‚å¸¸"""
        # PCAå˜æ¢
        scaled_data = self.scaler.transform(data.fillna(method='ffill').fillna(method='bfill'))
        pca_data = self.pca.transform(scaled_data)
        
        # é‡æ„è¯¯å·®
        reconstructed = self.pca.inverse_transform(pca_data)
        reconstruction_errors = np.mean((scaled_data - reconstructed) ** 2, axis=1)
        
        # å¼‚å¸¸åˆ¤å®š
        threshold = np.percentile(reconstruction_errors, (1 - self.contamination) * 100)
        pattern_anomalies = reconstruction_errors > threshold
        
        return pattern_anomalies
    
    def detect_anomalies(self, data: pd.DataFrame, 
                        correlation_window: int = 20) -> List[MultiMetricAnomaly]:
        """
        ç»¼åˆæ£€æµ‹å¤šæŒ‡æ ‡å¼‚å¸¸
        
        Args:
            data: å¤šæŒ‡æ ‡æ•°æ®
            correlation_window: ç›¸å…³æ€§åˆ†æçª—å£å¤§å°
        """
        if not self.fitted:
            self.fit(data)
        
        # å„ç±»å‹å¼‚å¸¸æ£€æµ‹
        correlation_anomalies = self.detect_correlation_anomalies(data, correlation_window)
        multivariate_anomalies, isolation_scores = self.detect_multivariate_outliers(data)
        pattern_anomalies = self.detect_pattern_anomalies(data)
        
        # åˆå¹¶å¼‚å¸¸ç»“æœ
        anomalies = []
        
        for i in range(len(data)):
            anomaly_types = []
            total_score = 0
            
            if correlation_anomalies[i]:
                anomaly_types.append('correlation')
                total_score += 0.3
            
            if multivariate_anomalies[i]:
                anomaly_types.append('outlier')
                total_score += 0.4
            
            if pattern_anomalies[i]:
                anomaly_types.append('pattern')
                total_score += 0.3
            
            if anomaly_types:
                # è®¡ç®—ä¸ªä½“æŒ‡æ ‡åˆ†æ•°
                individual_scores = {}
                for col in data.columns:
                    # åŸºäºZ-scoreè®¡ç®—ä¸ªä½“å¼‚å¸¸åˆ†æ•°
                    col_data = data[col].iloc[max(0, i-20):i+1]
                    if len(col_data) > 3:
                        z_score = abs(data[col].iloc[i] - col_data.mean()) / (col_data.std() + 1e-6)
                        individual_scores[col] = min(z_score / 3, 1.0)
                    else:
                        individual_scores[col] = 0.0
                
                # è¯†åˆ«å—å½±å“çš„æŒ‡æ ‡
                affected_metrics = [col for col, score in individual_scores.items() if score > 0.5]
                
                # è®¡ç®—ç›¸å…³æ€§å˜åŒ–
                correlation_changes = {}
                if i >= correlation_window:
                    window_data = data.iloc[i-correlation_window:i]
                    window_corr = window_data.corr()
                    for col in data.columns:
                        baseline_avg_corr = self.baseline_correlations[col].drop(col).mean()
                        window_avg_corr = window_corr[col].drop(col).mean()
                        correlation_changes[col] = abs(window_avg_corr - baseline_avg_corr)
                
                anomaly = MultiMetricAnomaly(
                    timestamp=data.index[i],
                    anomaly_score=min(total_score, 1.0),
                    affected_metrics=affected_metrics,
                    anomaly_type='+'.join(anomaly_types),
                    individual_scores=individual_scores,
                    correlation_changes=correlation_changes
                )
                
                anomalies.append(anomaly)
        
        return anomalies
    
    def plot_multimetric_analysis(self, data: pd.DataFrame, anomalies: List[MultiMetricAnomaly]):
        """å¯è§†åŒ–å¤šæŒ‡æ ‡åˆ†æç»“æœ"""
        fig = plt.figure(figsize=(16, 12))
        
        # åˆ›å»ºå­å›¾
        gs = fig.add_gridspec(3, 2, height_ratios=[2, 1, 1], hspace=0.3, wspace=0.3)
        
        # 1. ä¸»æ—¶é—´åºåˆ—å›¾
        ax1 = fig.add_subplot(gs[0, :])
        
        # ç»˜åˆ¶æ‰€æœ‰æŒ‡æ ‡
        colors = plt.cm.tab10(np.linspace(0, 1, len(data.columns)))
        for i, (col, color) in enumerate(zip(data.columns, colors)):
            ax1.plot(data.index, data[col], label=col, alpha=0.7, color=color)
        
        # æ ‡è®°å¼‚å¸¸ç‚¹
        if anomalies:
            for anomaly in anomalies:
                ax1.axvline(x=anomaly.timestamp, color='red', alpha=0.3, linestyle='--')
                
                # åœ¨å¼‚å¸¸ç‚¹ä½ç½®æ·»åŠ æ ‡æ³¨
                y_pos = ax1.get_ylim()[1] * 0.9
                ax1.annotate(f'Score: {anomaly.anomaly_score:.2f}',
                           xy=(anomaly.timestamp, y_pos),
                           xytext=(10, -10), textcoords='offset points',
                           bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7),
                           fontsize=8)
        
        ax1.set_title('Multi-Metric Time Series with Anomalies')
        ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        ax1.grid(True, alpha=0.3)
        
        # 2. ç›¸å…³æ€§çƒ­åŠ›å›¾
        ax2 = fig.add_subplot(gs[1, 0])
        sns.heatmap(self.baseline_correlations, annot=True, cmap='coolwarm', 
                   center=0, ax=ax2, cbar_kws={'shrink': 0.8})
        ax2.set_title('Baseline Correlations')
        
        # 3. PCAè§£é‡Šæ–¹å·®
        ax3 = fig.add_subplot(gs[1, 1])
        if self.fitted:
            explained_variance = self.pca.explained_variance_ratio_
            cumsum_variance = np.cumsum(explained_variance)
            
            ax3.bar(range(1, len(explained_variance) + 1), explained_variance, 
                   alpha=0.7, label='Individual')
            ax3.plot(range(1, len(cumsum_variance) + 1), cumsum_variance, 
                    'ro-', label='Cumulative')
            ax3.set_xlabel('Principal Component')
            ax3.set_ylabel('Explained Variance Ratio')
            ax3.set_title('PCA Explained Variance')
            ax3.legend()
            ax3.grid(True, alpha=0.3)
        
        # 4. å¼‚å¸¸åˆ†ç±»ç»Ÿè®¡
        ax4 = fig.add_subplot(gs[2, :])
        
        if anomalies:
            # ç»Ÿè®¡å¼‚å¸¸ç±»å‹
            anomaly_types = {}
            for anomaly in anomalies:
                for atype in anomaly.anomaly_type.split('+'):
                    anomaly_types[atype] = anomaly_types.get(atype, 0) + 1
            
            # ç»˜åˆ¶å¼‚å¸¸ç±»å‹åˆ†å¸ƒ
            types = list(anomaly_types.keys())
            counts = list(anomaly_types.values())
            
            bars = ax4.bar(types, counts, alpha=0.7, color=['skyblue', 'lightcoral', 'lightgreen'])
            ax4.set_title('Anomaly Type Distribution')
            ax4.set_ylabel('Count')
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, count in zip(bars, counts):
                height = bar.get_height()
                ax4.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                        f'{count}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig('multimetric_methods/multimetric_anomaly_analysis.png', 
                   dpi=300, bbox_inches='tight')
        plt.show()
    
    def generate_summary_report(self, anomalies: List[MultiMetricAnomaly]) -> str:
        """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
        if not anomalies:
            return "æœªæ£€æµ‹åˆ°å¼‚å¸¸"
        
        report = []
        report.append("ğŸ“Š å¤šæŒ‡æ ‡å¼‚å¸¸æ£€æµ‹æŠ¥å‘Š")
        report.append("=" * 60)
        report.append(f"æ£€æµ‹åˆ°å¼‚å¸¸æ€»æ•°: {len(anomalies)}")
        report.append("")
        
        # æŒ‰ç±»å‹ç»Ÿè®¡
        type_stats = {}
        for anomaly in anomalies:
            for atype in anomaly.anomaly_type.split('+'):
                type_stats[atype] = type_stats.get(atype, 0) + 1
        
        report.append("å¼‚å¸¸ç±»å‹åˆ†å¸ƒ:")
        for atype, count in type_stats.items():
            report.append(f"  {atype}: {count}")
        report.append("")
        
        # å—å½±å“æŒ‡æ ‡ç»Ÿè®¡
        metric_impact = {}
        for anomaly in anomalies:
            for metric in anomaly.affected_metrics:
                metric_impact[metric] = metric_impact.get(metric, 0) + 1
        
        if metric_impact:
            report.append("å—å½±å“æŒ‡æ ‡é¢‘æ¬¡:")
            sorted_metrics = sorted(metric_impact.items(), key=lambda x: x[1], reverse=True)
            for metric, count in sorted_metrics:
                report.append(f"  {metric}: {count} æ¬¡")
        
        report.append("")
        
        # ä¸¥é‡å¼‚å¸¸ç‚¹
        severe_anomalies = [a for a in anomalies if a.anomaly_score > 0.7]
        if severe_anomalies:
            report.append(f"ä¸¥é‡å¼‚å¸¸ç‚¹ (åˆ†æ•° > 0.7): {len(severe_anomalies)}")
            for anomaly in severe_anomalies[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
                report.append(f"  {anomaly.timestamp}: åˆ†æ•°={anomaly.anomaly_score:.3f}, "
                            f"ç±»å‹={anomaly.anomaly_type}, "
                            f"å½±å“æŒ‡æ ‡={','.join(anomaly.affected_metrics)}")
        
        return "\n".join(report)

# ç¤ºä¾‹ç”¨æ³•
if __name__ == "__main__":
    # ç”Ÿæˆå¤šæŒ‡æ ‡ç¤ºä¾‹æ•°æ®
    np.random.seed(42)
    dates = pd.date_range('2024-01-01', periods=200, freq='10min')
    
    # åˆ›å»ºç›¸å…³çš„å¤šä¸ªæŒ‡æ ‡
    base_cpu = 30 + 15 * np.sin(2 * np.pi * np.arange(200) / 144)
    cpu_usage = base_cpu + np.random.normal(0, 3, 200)
    
    # å†…å­˜ä½¿ç”¨ç‡ï¼ˆä¸CPUæ­£ç›¸å…³ï¼‰
    memory_usage = 40 + 0.6 * cpu_usage + np.random.normal(0, 4, 200)
    
    # ç½‘ç»œI/Oï¼ˆä¸CPUè´Ÿç›¸å…³ï¼‰
    network_io = 100 - 0.4 * cpu_usage + np.random.normal(0, 5, 200)
    
    # ç£ç›˜I/Oï¼ˆç‹¬ç«‹å˜åŒ–ï¼‰
    disk_io = 50 + 20 * np.sin(2 * np.pi * np.arange(200) / 100) + np.random.normal(0, 6, 200)
    
    # æ’å…¥å¤šç±»å‹å¼‚å¸¸
    # 1. å•æŒ‡æ ‡å¼‚å¸¸
    cpu_usage[50] = 90
    
    # 2. å¤šæŒ‡æ ‡å¼‚å¸¸ï¼ˆç›¸å…³æ€§å¼‚å¸¸ï¼‰
    cpu_usage[100] = 80
    memory_usage[100] = 30  # ç ´åæ­£ç›¸å…³æ€§
    
    # 3. æ¨¡å¼å¼‚å¸¸
    cpu_usage[150:155] += 25
    memory_usage[150:155] += 30
    network_io[150:155] -= 20
    
    # åˆ›å»ºDataFrame
    data = pd.DataFrame({
        'CPU_Usage': cpu_usage,
        'Memory_Usage': memory_usage,
        'Network_IO': network_io,
        'Disk_IO': disk_io
    }, index=dates)
    
    # åˆ›å»ºå¤šæŒ‡æ ‡å¼‚å¸¸æ£€æµ‹å™¨
    detector = MultiMetricAnomalyDetector(contamination=0.05)
    
    # æ£€æµ‹å¼‚å¸¸
    anomalies = detector.detect_anomalies(data)
    
    # ç”ŸæˆæŠ¥å‘Š
    print(detector.generate_summary_report(anomalies))
    print("\n" + "="*60)
    
    # è¯¦ç»†å¼‚å¸¸ä¿¡æ¯
    print("\nğŸ” è¯¦ç»†å¼‚å¸¸ä¿¡æ¯:")
    for i, anomaly in enumerate(anomalies[:10]):  # æ˜¾ç¤ºå‰10ä¸ª
        print(f"\nå¼‚å¸¸ {i+1}:")
        print(f"  æ—¶é—´: {anomaly.timestamp}")
        print(f"  åˆ†æ•°: {anomaly.anomaly_score:.3f}")
        print(f"  ç±»å‹: {anomaly.anomaly_type}")
        print(f"  å½±å“æŒ‡æ ‡: {anomaly.affected_metrics}")
        print(f"  ä¸ªä½“åˆ†æ•°: {anomaly.individual_scores}")
    
    # å¯è§†åŒ–åˆ†æ
    detector.plot_multimetric_analysis(data, anomalies)
    
    print(f"\nâœ… æ€»å…±æ£€æµ‹åˆ° {len(anomalies)} ä¸ªå¤šæŒ‡æ ‡å¼‚å¸¸")
