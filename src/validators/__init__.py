"""
éªŒè¯å™¨æ¨¡å—

åŒ…å«æ€§èƒ½éªŒè¯ã€å¯¹æ¯”åˆ†æç­‰åŠŸèƒ½
"""
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import roc_curve, auc
from ..scorers import OriginalScorer, RandomForestScorer, NeuralNetworkScorer
from ..utils import MetricsCalculator


class ScorerComparator:
    """è¯„åˆ†å™¨å¯¹æ¯”éªŒè¯ç±»"""
    
    def __init__(self, data):
        self.data = data
        self.scorers = {
            'Original': OriginalScorer(),
            'RandomForest': RandomForestScorer(),
            'NeuralNetwork': NeuralNetworkScorer()
        }
        self.results = {}
        
    def train_and_evaluate(self, train_ratio=0.7):
        """è®­ç»ƒå¹¶è¯„ä¼°æ‰€æœ‰è¯„åˆ†å™¨"""
        # åˆ†å‰²æ•°æ®
        train_size = int(train_ratio * len(self.data))
        train_data = self.data.iloc[:train_size].copy()
        test_data = self.data.iloc[train_size:].copy()
        
        true_labels = test_data['is_anomaly'].values
        
        for name, scorer in self.scorers.items():
            print(f"ğŸ”§ è®­ç»ƒ {name} è¯„åˆ†å™¨...")
            
            # è®­ç»ƒ
            scorer.fit(train_data)
            
            # é¢„æµ‹
            scores = scorer.score(test_data)
            
            # è¯„ä¼°
            metrics = MetricsCalculator.evaluate_performance(true_labels, scores)
            
            self.results[name] = {
                'scorer': scorer,
                'scores': scores,
                'metrics': metrics
            }
            
            print(f"   - Precision: {metrics['precision']:.3f}")
            print(f"   - Recall: {metrics['recall']:.3f}")
            print(f"   - F1-Score: {metrics['f1']:.3f}")
            print(f"   - AUC: {metrics['auc']:.3f}")
            print()
            
        return self.results
    
    def plot_comparison(self, figsize=(15, 10)):
        """ç»˜åˆ¶å¯¹æ¯”å›¾"""
        if not self.results:
            raise ValueError("è¯·å…ˆè¿è¡Œ train_and_evaluate()")
            
        fig, axes = plt.subplots(2, 2, figsize=figsize)
        fig.suptitle('è¯„åˆ†å™¨æ€§èƒ½å¯¹æ¯”', fontsize=16, fontweight='bold')
        
        # 1. è¯„åˆ†åˆ†å¸ƒå¯¹æ¯”
        ax1 = axes[0, 0]
        for name, result in self.results.items():
            ax1.hist(result['scores'], bins=30, alpha=0.6, label=name)
        ax1.set_title('è¯„åˆ†åˆ†å¸ƒå¯¹æ¯”')
        ax1.set_xlabel('å¼‚å¸¸è¯„åˆ†')
        ax1.set_ylabel('é¢‘æ¬¡')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. ROCæ›²çº¿å¯¹æ¯”
        ax2 = axes[0, 1]
        true_labels = self.data.iloc[int(0.7*len(self.data)):]['is_anomaly'].values
        
        for name, result in self.results.items():
            if len(np.unique(true_labels)) > 1:
                fpr, tpr, _ = roc_curve(true_labels, result['scores'])
                auc_score = auc(fpr, tpr)
                ax2.plot(fpr, tpr, label=f'{name} (AUC={auc_score:.3f})')
        
        ax2.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='éšæœºçŒœæµ‹')
        ax2.set_title('ROCæ›²çº¿å¯¹æ¯”')
        ax2.set_xlabel('å‡æ­£ç‡ (FPR)')
        ax2.set_ylabel('çœŸæ­£ç‡ (TPR)')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”
        ax3 = axes[1, 0]
        metrics_df = pd.DataFrame({
            name: [result['metrics']['precision'], 
                   result['metrics']['recall'], 
                   result['metrics']['f1']]
            for name, result in self.results.items()
        }, index=['Precision', 'Recall', 'F1-Score'])
        
        metrics_df.plot(kind='bar', ax=ax3)
        ax3.set_title('æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”')
        ax3.set_ylabel('å¾—åˆ†')
        ax3.legend()
        ax3.tick_params(axis='x', rotation=0)
        
        # 4. ç‰¹å¾é‡è¦æ€§ (ä»…Random Forest)
        ax4 = axes[1, 1]
        if 'RandomForest' in self.results:
            rf_scorer = self.results['RandomForest']['scorer']
            importance_df = rf_scorer.get_feature_importance().head(10)
            bars = ax4.barh(range(len(importance_df)), importance_df['importance'])
            ax4.set_yticks(range(len(importance_df)))
            ax4.set_yticklabels(importance_df['feature'], fontsize=9)
            ax4.set_title('Random Forestç‰¹å¾é‡è¦æ€§')
            ax4.set_xlabel('é‡è¦æ€§')
        
        plt.tight_layout()
        plt.show()
        
    def generate_report(self):
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        if not self.results:
            raise ValueError("è¯·å…ˆè¿è¡Œ train_and_evaluate()")
            
        print("=" * 60)
        print("ğŸ“Š è¯„åˆ†å™¨æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š")
        print("=" * 60)
        
        # æ€§èƒ½å¯¹æ¯”è¡¨
        metrics_df = pd.DataFrame({
            name: result['metrics'] 
            for name, result in self.results.items()
        }).T
        
        print("\nğŸ¯ æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”:")
        print(metrics_df[['precision', 'recall', 'f1', 'auc']].round(3))
        
        # æœ€ä½³æ€§èƒ½
        best_f1 = metrics_df['f1'].idxmax()
        best_auc = metrics_df['auc'].idxmax()
        
        print(f"\nğŸ† æœ€ä½³F1-Score: {best_f1} ({metrics_df.loc[best_f1, 'f1']:.3f})")
        print(f"ğŸ† æœ€ä½³AUC: {best_auc} ({metrics_df.loc[best_auc, 'auc']:.3f})")
        
        # æ”¹è¿›å»ºè®®
        print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
        if 'RandomForest' in self.results:
            rf_precision = self.results['RandomForest']['metrics']['precision']
            orig_precision = self.results['Original']['metrics']['precision']
            improvement = rf_precision - orig_precision
            print(f"   - Random Forestç›¸æ¯”åŸå§‹æ–¹æ³•Precisionæå‡: {improvement:+.3f}")
            
        print("   - å»ºè®®åœ¨ç”Ÿäº§ç¯å¢ƒä¸­A/Bæµ‹è¯•éªŒè¯æ•ˆæœ")
        print("   - å¯è€ƒè™‘é›†æˆå¤šä¸ªæ¨¡å‹è¿›ä¸€æ­¥æå‡æ€§èƒ½")
        
        return metrics_df
